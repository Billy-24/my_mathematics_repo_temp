{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Billy-24/my_mathematics_repo_temp/blob/main/Copie_de_la_version_definitive_du_Projet_wintage_version_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSz6DBWcEl2T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import io\n",
        "uploaded = files.upload()\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Vp9v0oYmsJJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# L'analyse exploratoire des données\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/winemag-data-130k-v2.csv\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "4GFOp0YyFAe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# type de variable du dataset\n",
        "data.info()"
      ],
      "metadata": {
        "id": "KYIsYwj4FBnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.winery .nunique()"
      ],
      "metadata": {
        "id": "vz44sw6pFI9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "gXUJPzuPFJuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supprimer les colonnes non pertinentes\n",
        "data = data.drop(['Unnamed: 0','region_2', 'description', 'taster_twitter_handle'], axis=1)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "jabcKhUaFPe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe(include=\"all\")"
      ],
      "metadata": {
        "id": "NpL_Z2YzFSsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vérification des valeurs manquantes dans le dataset\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.heatmap(data.isna(), cbar=False)"
      ],
      "metadata": {
        "id": "Zl2_H6p7FV8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification des doublons\n",
        "duplicatas = data[data.duplicated()]\n",
        "print(\"Enregistrements Dupliqués :\", duplicatas.shape)"
      ],
      "metadata": {
        "id": "Yq3iqWuTFY3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#supprimons les doublons\n",
        "data.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "PYHLNFjbFbN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "4FVCe4cHFeEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# L'ensemble de données présente toutes les variables prédictives comme catégoriques, à l'exception du « prix ». La variable dépendante est « points ».cat_data=[]\n",
        "# Sectionner les catégories des valeur numériques\n",
        "cat_data=[]\n",
        "num_data=[]\n",
        "for i,c in enumerate(data.dtypes):\n",
        "  if c==object:\n",
        "    cat_data.append(data.iloc[:,i])\n",
        "  else:\n",
        "    num_data.append(data.iloc[:,i])\n",
        "cat_data=pd.DataFrame(cat_data)\n",
        "cat_data"
      ],
      "metadata": {
        "id": "ZkAXaFDQFjH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transposer les valeurs manquantes sectionnées\n",
        "cat_data=[]\n",
        "num_data=[]\n",
        "for i,c in enumerate(data.dtypes):\n",
        "  if c==object:\n",
        "    cat_data.append(data.iloc[:,i])\n",
        "  else:\n",
        "    num_data.append(data.iloc[:,i])\n",
        "cat_data=pd.DataFrame(cat_data).transpose()\n",
        "num_data=pd.DataFrame(num_data).transpose()\n",
        "cat_data"
      ],
      "metadata": {
        "id": "QyxGH0v0Fm3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pour les valeurs manquantes dans les variables cathégoriques, on doit les remplacer par le Mode\n",
        "cat_data=cat_data.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
        "cat_data.isnull().sum().any()"
      ],
      "metadata": {
        "id": "GYbH4JHbFr-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse des valeurs numériques\n",
        "data[['points','price']].describe()"
      ],
      "metadata": {
        "id": "VYxyAaONGeDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyser la correlation entre price et points\n",
        "data[['price','points']].corr(method = 'pearson')"
      ],
      "metadata": {
        "id": "kBR55-v6GhcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carte de corrélation\n",
        "plt.figure(figsize=(2,2),)\n",
        "correlacion = data.corr()\n",
        "sns.heatmap(correlacion,annot=True)"
      ],
      "metadata": {
        "id": "XSzeTCE2GlCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suite à ce résultat 'il existe une corrélation positive modérée entre 'price' et 'points' dans vos données. Cela suggère qu'il y a une tendance générale selon laquelle des vins avec des prix plus élevés ont tendance à obtenir des points plus élevés, et vice versa. Cependant, d'autres facteurs peuvent également influencer la notation des vins, car la corrélation n'est pas parfaite."
      ],
      "metadata": {
        "id": "ipnI-vzRHCNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse country - points\n",
        "data[['points','country']].describe()"
      ],
      "metadata": {
        "id": "Mq-uMVSRHGGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_countries = data.groupby(['country'])['points'].mean().sort_values(ascending=False).head(10)\n",
        "print(top_countries)"
      ],
      "metadata": {
        "id": "yNpssMjlHKxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse des variables numériques\n",
        "# Analyse univariée des valeurs numériques : Créez un nuage de points avec 'price' sur l'axe des x\n",
        "plt.scatter(range(len(data['price'])), data['price'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B-Y4f-QUHQP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Étape 1 : Supprimer la valeur aberrante dans 'price'\n",
        "seuil_aberrant = 3200\n",
        "num_data = num_data[num_data['price'] <= seuil_aberrant]"
      ],
      "metadata": {
        "id": "yA0UClPdIGqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nuage de points après suppression de la valeur aberrante\n",
        "plt.scatter(range(len(num_data['price'])), num_data['price'])\n",
        "plt.title('Nuage de points après suppression de la valeur aberrante')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4S1qguLzIPG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Étape 2 : Remplacer les valeurs manquantes dans 'price' par la moyenne\n",
        "moyenne_price = num_data['price'].mean()\n",
        "num_data['price'].fillna(moyenne_price, inplace=True)\n",
        "num_data"
      ],
      "metadata": {
        "id": "zzc8ctYzIWWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Un test pour voir si on retrouve encore des NAN dans le dataset\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.heatmap(num_data.isna(), cbar=False)"
      ],
      "metadata": {
        "id": "yk9SlvoKK_Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fusionner les ensembles de données en fonction des colonnes\n",
        "combined_data = pd.concat([cat_data, num_data], axis=1)\n",
        "\n",
        "# Afficher le résultat\n",
        "combined_data\n"
      ],
      "metadata": {
        "id": "OqPVxu0iLTu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remplacer les NaN par la moyenne dans les colonnes 'points' et 'price'\n",
        "combined_data['points'].fillna(combined_data['points'].mean(), inplace=True)\n",
        "combined_data['price'].fillna(combined_data['price'].mean(), inplace=True)\n",
        "# Afficher le résultat\n",
        "combined_data"
      ],
      "metadata": {
        "id": "DePkEJk_UCWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Un test pour voir si on retrouve encore des NAN dans le dataset\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.heatmap(combined_data.isna(), cbar=False)"
      ],
      "metadata": {
        "id": "0aGvc5JRUO1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Training pipeline ###\n",
        "print(\"--- Training pipeline ---\")\n",
        "print()\n",
        "\n",
        "# Encoding categorical features and standardizing numeric features\n",
        "\n",
        "print(\"#### X_train BEFORE preprocessing ####\")\n",
        "display(combined_data.head())\n",
        "print()\n",
        "\n",
        "print(\"Encoding categorical features and standardizing numerical features...\")\n",
        "## First let's import libraries\n",
        "## StandardScaler to scale data (i.e apply Z-score)\n",
        "## OneHotEncoder to encode categorical variables\n",
        "numeric_features = ['points', '\tprice'] # Choose which column index we are going to scale\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_features = ['country',\t'designation',\t'province',\t'region_1',\t'taster_name', 'variety', 'winery\t'] # Choose which column index we are going to encode\n",
        "categorical_transformer = OneHotEncoder(drop=\"first\")\n",
        "# Apply ColumnTransformer to create a pipeline that will apply the above preprocessing\n",
        "feature_encoder = ColumnTransformer(transformers=[('cat', categorical_transformer, categorical_features), ('num', numeric_transformer, numeric_features)])\n"
      ],
      "metadata": {
        "id": "NFGg3XL9dzCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline de prétraitement des données qui encode les variables catégorielles et normalise les variables numériques, puis applique cette pipeline aux données d'entraînement. Cela prépare les données pour être utilisées dans un modèle d'apprentissage automatique."
      ],
      "metadata": {
        "id": "s0F-2dbtf4O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding categorical features and standardizing numeric features\n",
        "\n",
        "print(\"#### X_train BEFORE preprocessing ####\")\n",
        "display(combined_data.head())\n",
        "print()\n",
        "\n",
        "print(\"Encoding categorical features and standardizing numerical features...\")\n",
        "## First let's import libraries\n",
        "## StandardScaler to scale data (i.e apply Z-score)\n",
        "## OneHotEncoder to encode categorical variables\n",
        "numeric_features = ['points', 'price']  # Choose which column index we are going to scale\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_features = ['country', 'designation', 'province', 'region_1', 'taster_name', 'variety', 'winery']  # Choose which column index we are going to encode\n",
        "categorical_transformer = OneHotEncoder(drop=\"first\")\n",
        "\n",
        "# Apply ColumnTransformer to create a pipeline that will apply the above preprocessing\n",
        "feature_encoder = ColumnTransformer(transformers=[('cat', categorical_transformer, categorical_features), ('num', numeric_transformer, numeric_features)])\n",
        "\n",
        "# Apply the pipeline to combined_data and store the result in X_train_preprocessed\n",
        "X_train_preprocessed = feature_encoder.fit_transform(combined_data)\n",
        "\n",
        "#Display (the first few rows of the preprocessed data)\n",
        "print(\"#### X_train AFTER preprocessing ####\")\n",
        "display(X_train_preprocessed.toarray())\n"
      ],
      "metadata": {
        "id": "PQxQLLhNly0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_preprocessed.toarray().shape"
      ],
      "metadata": {
        "id": "NUpY4MyzrhCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply ColumnTransformer to create a pipeline that will apply the above preprocessing\n",
        "feature_encoder = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(), categorical_features),  # Remove drop=\"first\"\n",
        "        ('num', numeric_transformer, numeric_features)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "lvUPM0hXGyvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Sur la base de notre DataFrame\n",
        "X = combined_data[['country', 'designation', 'province', 'region_1', 'taster_name', 'variety', 'winery', 'price']]  # Je n'inclus pas 'points' ici car c'est la variable cible\n",
        "y = combined_data['points']\n",
        "\n",
        "\n",
        "# Définissez les transformations\n",
        "numeric_features = ['price']\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_features = ['country', 'designation', 'province', 'region_1', 'taster_name', 'variety', 'winery']\n",
        "categorical_transformer = OneHotEncoder(drop=\"first\")\n",
        "\n",
        "# Appliquer le ColumnTransformer à vos données d'entraînement\n",
        "feature_encoder = ColumnTransformer(transformers=[('cat', categorical_transformer, categorical_features),\n",
        " ('num', numeric_transformer, numeric_features)])\n",
        "X_encoded = feature_encoder.fit_transform(X)\n",
        "\n",
        "# Obtention des noms de colonnes après le prétraitement\n",
        "feature_names_out = feature_encoder.get_feature_names_out(input_features=categorical_features + numeric_features)\n",
        "\n",
        "# Affichez les dix premiers noms de colonnes\n",
        "print(feature_names_out[:10])\n"
      ],
      "metadata": {
        "id": "CHOgEoAT-bxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher les noms de colonnes obtenus après la transformation\n",
        "print(feature_names_out)"
      ],
      "metadata": {
        "id": "uScBpgrWHhBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divisons l'ensemble de données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Affichons les dimensions des ensembles d'entraînement et de test\n",
        "print(\"Dimensions de l'ensemble d'entraînement X :\", X_train.shape)\n",
        "print(\"Dimensions de l'ensemble de test X :\", X_test.shape)\n",
        "print(\"Dimensions de l'ensemble d'entraînement y :\", y_train.shape)\n",
        "print(\"Dimensions de l'ensemble de test y :\", y_test.shape)"
      ],
      "metadata": {
        "id": "nnvXUOpl9Udx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ça c'est pour le radom forest c'est à modifier pour mettre la regression linéaire\n",
        "#Créez et entraînez le modèle de régression linéaire\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "MpUi9tTet5lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Créez et entraînez le modèle de régression linéaire\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "_KMnNBmQzach"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error\n",
        "# Faire des prédictions sur l'ensemble de test\n",
        "y_pred = model.predict(X_test)\n",
        "# Calculer les métriques de performance sur l'ensemble de test\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "medae = median_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "# Afficher les métriques de performance sur l'ensemble de test\n",
        "print(f'MAE test : {mae}')\n",
        "print(f'MedAE test : {medae}')\n",
        "print(f'MSE test : {mse}')\n",
        "print(f'RMSE test : {rmse}')\n",
        "print(f'R² test : {r2}')\n"
      ],
      "metadata": {
        "id": "_LKNVxScua3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Interprétation des métriques pour le modèle de régression\n",
        "## Mean Absolute Error (MAE) test : 0.6155\n",
        "\"En moyenne, les prédictions du modèle diffèrent d'environ 0.6155 points de la vérité terrain. Cela signifie que, en moyenne, le modèle a une erreur d'environ 0.6155 points par rapport aux valeurs réelles.\n",
        "\n",
        "## Median Absolute Error (MedAE) test : 0.4869\n",
        "\"La moitié des prédictions du modèle sur l'ensemble de test diffèrent de 0.4869 points ou moins de la vérité terrain. Cela suggère que le modèle a une précision médiane relativement bonne.\n",
        "\n",
        "## Mean Squared Error (MSE) test : 0.6454\n",
        "\"En moyenne, l'erreur quadratique entre les prédictions et les valeurs réelles sur l'ensemble de test est d'environ 0.6454 points. Cela indique que, en général, les prédictions s'écartent peu des valeurs réelles.\n",
        "\n",
        "## Root Mean Squared Error (RMSE) test : 0.8034\n",
        "\"La racine carrée du MSE est d'environ 0.8034 points. Une valeur faible de RMSE montre que le modèle fait de bonnes prédictions en termes d'écart absolu moyen par rapport aux valeurs réelles.\n",
        "\n",
        "## R-squared (R²) test : 0.3538\n",
        "\"Le R² est d'environ 0.3538, ce qui signifie que le modèle explique environ 35.38 % de la variance dans la variable cible. Bien que le modèle apporte une certaine explication, il reste de la variance non expliquée.\n",
        "\n",
        "# En conclusion, bien que le modèle ait une certaine capacité prédictive, il y a encore de la place pour l'amélioration, surtout si vous cherchez à réduire davantage l'erreur de prédiction"
      ],
      "metadata": {
        "id": "pzEinsZ0FWOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation des prédictions par rapport aux valeurs réelles\n",
        "# Diagramme de Dispersion\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=y_test, y=y_pred)\n",
        "plt.title('Diagramme de Dispersion')\n",
        "plt.xlabel('Valeurs Réelles')\n",
        "plt.ylabel('Prédictions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JsrF_BShZKZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogramme des Erreurs\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(errors.to_numpy(), bins=50, kde=True)\n",
        "plt.title('Histogramme des Erreurs')\n",
        "plt.xlabel('Erreurs')\n",
        "plt.ylabel('Fréquence')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZzojtvF1aSYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Position du pic: Nous constatons que l'histogramme est centré autour de zéro, cela signifie que notre modèle a tendance à faire des prédictions proches des valeurs réelles."
      ],
      "metadata": {
        "id": "N34J2ltpeIeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Courbe de Régression\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.regplot(x=y_test, y=y_pred, scatter_kws={'s': 20, 'alpha': 0.3})\n",
        "plt.title('Courbe de Régression')\n",
        "plt.xlabel('Valeurs Réelles')\n",
        "plt.ylabel('Prédictions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J8sD2Mm9avTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On observe une dispersion importante autour de la ligne de régression ce qui indique une variabilité élevée dans les prédictions."
      ],
      "metadata": {
        "id": "54aJgxRYgT30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagramme de Résidus\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.residplot(x=y_test, y=errors, lowess=True, scatter_kws={'s': 20, 'alpha': 0.3})\n",
        "plt.title('Diagramme de Résidus')\n",
        "plt.xlabel('Valeurs Réelles')\n",
        "plt.ylabel('Résidus')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W49cxEg0gyJw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}